# -*- coding: utf-8 -*-
"""Projects.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GFpH2tNWr2sNQ9Y2TY3uiYNaUlg1_XBy
"""

#Import Libraries
import numpy as np 
import pandas as pd 
import sklearn
from sklearn.feature_extraction.text import CountVectorizer
import itertools
import matplotlib.pyplot as plt
import seaborn as sns

#Load the data
fake = pd.read_csv('Fake.csv')
real = pd.read_csv('True.csv')

fake.shape

real.shape

fake.info()

fake.isnull().sum()

fake.duplicated().sum()

real.info()

fake.isnull().sum()

fake.duplicated().sum()

real.info()

real.isnull().sum()

real.duplicated().sum()

fake_1 = fake.drop_duplicates()
real_1 = real.drop_duplicates()

fake_1.duplicated().sum()

real_1.duplicated().sum()

#Adding category column
fake_1['Real'] = 0
real_1['Real'] = 1

#Combining real and fake
Data = pd.concat([fake_1, real_1], ignore_index=True, sort=False)

#Shuffle Dataset
from sklearn.utils import shuffle
Data = shuffle(Data)
Data = Data.reset_index(drop = True)

Data.info()

#Class distribution
import seaborn as sb
print(Data.groupby(['Real'])['Real'].count())
plt.title("Distribution of classes")
sb.countplot(x='Real', data=Data)

#Split of data by subject
f = plt.figure()
plt.title("Count of news categories")
f.set_figwidth(8)
f.set_figheight(5)
plt.xticks(rotation = 90)
sb.countplot(x = "subject", data =  Data)
plt.show()

#World Cloud of important words in each category
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from wordcloud import WordCloud, STOPWORDS

#Separate the label column
y = Data.Real
x = Data.text

#Create Train and Test datasets
from sklearn.model_selection import train_test_split  
X_train, X_test, y_train, y_test = train_test_split(x, y, stratify=y,test_size = 0.33,random_state=0)
print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)

#Vectorizer for converting text to numeric values
from sklearn.feature_extraction.text import TfidfVectorizer
tfidf_vevtorizer = TfidfVectorizer(stop_words='english', max_df=0.7)
tfidf_train = tfidf_vevtorizer.fit_transform(X_train)
tfidf_test = tfidf_vevtorizer.transform(X_train)

#Functon for Confusion Matrix

import matplotlib.pyplot as plt
import itertools
def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')
        thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")
    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

#Libraries for model building
from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import MultinomialNB
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.linear_model import PassiveAggressiveClassifier

#Decision Tree
from sklearn.tree import DecisionTreeClassifier
DT_pipeline = Pipeline([
                  ('linear',tfidf_vevtorizer),
                  ('DT_clf',DecisionTreeClassifier())])
DT_pipeline.fit(X_train,y_train)

predict_DT = DT_pipeline.predict(X_test) 
score_DT = metrics.accuracy_score(y_test,predict_DT)
print(f' Testing Accuracy: {round(score_DT*100,2)}%')

predict_DT_tr = DT_pipeline.predict(X_train) 
score_DT_tr = metrics.accuracy_score(y_train,predict_DT_tr)
print(f' Training Accuracy: {round(score_DT_tr*100,2)}%')

#Confusion Matrix for DT
cm = metrics.confusion_matrix(y_test, predict_DT)
plot_confusion_matrix(cm, classes=['Fake', 'Real'])

print(metrics.classification_report(y_test,predict_DT))

import pickle
pickle_out = open("projects.pkl","wb")
pickle.dump(DT_pipeline, pickle_out)
pickle_out.close()